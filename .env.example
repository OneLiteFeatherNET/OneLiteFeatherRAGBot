# Copy to .env and fill in values. This file documents required variables.

# --- Discord Bot ---
# Discord Bot Token (create in Discord Developer Portal)
APP_DISCORD_TOKEN=
# Optional: custom presence text shown in Discord
APP_BOT_STATUS=OneLiteFeather RAG
# Optional: limit slash-command sync to specific guilds (JSON array, e.g. [123456789012345678])
APP_GUILD_IDS=
# Enable mention/reply triggers (requires enabling "Message Content Intent" in Discord Developer Portal)
APP_ENABLE_MESSAGE_CONTENT_INTENT=false
APP_CONFIG_BACKEND=db  # db (default) or file

# --- RAG behavior ---
# If true, when no vectors are indexed yet, the bot answers via plain LLM
APP_RAG_FALLBACK_TO_LLM=true
# If true, when RAG finds no sources, include a general LLM answer appended to the RAG answer
APP_RAG_MIX_LLM_WITH_RAG=false
# If mixing is enabled, choose thresholding:
# When APP_RAG_SCORE_KIND=similarity (higher better), mix if best_score < threshold
# When APP_RAG_SCORE_KIND=distance (lower better), mix if best_score > threshold
APP_RAG_MIX_THRESHOLD=
APP_RAG_SCORE_KIND=similarity

# --- OpenAI ---
# OpenAI API Key used by the RAG service for LLM + embeddings
OPENAI_API_KEY=

# --- AI Provider ---
# Choose provider: openai (default) or ollama
APP_AI_PROVIDER=openai
# LLM + Embedding model names (provider-specific)
APP_LLM_MODEL=gpt-4.1-mini
APP_EMBED_MODEL=text-embedding-3-small
APP_TEMPERATURE=0.1
# Optional: set a system prompt for the LLM (also configurable via /config system_prompt_set)
APP_LLM_SYSTEM_PROMPT=

# Optional: choose embedding backend explicitly (openai, ollama, or vllm)
APP_EMBED_PROVIDER=openai

# Ollama specific (only used when APP_AI_PROVIDER=ollama)
# When using docker-compose with the provided service, use http://ollama:11434
APP_OLLAMA_BASE_URL=http://localhost:11434

# vLLM (OpenAI-compatible) provider
# Typical base URL: http://localhost:8000/v1 (used for LLM and, if APP_EMBED_PROVIDER=vllm, also for embeddings)
APP_VLLM_BASE_URL=http://localhost:8000/v1
# Some deployments require a key; otherwise leave empty
APP_VLLM_API_KEY=

# --- Postgres / pgvector (required for RAG API and indexing) ---
APP_PG_HOST=localhost
APP_PG_PORT=5432
APP_PG_USER=postgres
APP_PG_PASSWORD=postgres
APP_PG_DATABASE=postgres

# --- RAG settings ---
# Name of the pgvector table used for chunks
APP_TABLE_NAME=rag_chunks
# Embedding dimension for the selected embedding model (text-embedding-3-small -> 1536)
APP_EMBED_DIM=1536
# Top-k similar chunks to retrieve
APP_TOP_K=6

# --- Indexing CLI ---
# Trigger indexing via CLI, e.g.:
# uv run rag-index /path/to/repo https://github.com/ORG/repo
# or with Docker: docker compose run --rm bot rag-index /data/repos/my-repo https://github.com/ORG/my-repo

# --- Logging ---
# Default INFO; options: DEBUG, INFO, WARNING, ERROR
APP_LOG_LEVEL=INFO

# --- Ingestion defaults ---
# Default file extensions to include when none are provided by commands/config
# Provide as JSON array, e.g. [".md", ".py", ".yml", ".yaml", ".toml", ".json", ".txt"]
APP_INGEST_EXTS=[".md", ".py", ".yml", ".yaml", ".toml", ".json", ".txt"]

# --- Queue/ETL config ---
# Interval for Discord job status polling (seconds)
APP_QUEUE_WATCH_POLL_SEC=5.0
# Directory for ETL artifacts (manifests, prompts). Should not be committed.
APP_ETL_STAGING_DIR=.staging
APP_ETL_STAGING_BACKEND=local  # local or s3

# S3 staging (used when APP_ETL_STAGING_BACKEND=s3)
APP_S3_STAGING_BUCKET=
APP_S3_STAGING_PREFIX=rag-artifacts
APP_S3_REGION=
APP_S3_ENDPOINT_URL=
APP_S3_ACCESS_KEY_ID=
APP_S3_SECRET_ACCESS_KEY=

# --- RAG mode & gating ---
# Use 'auto' to let the bot decide when to use RAG; 'rag' to always use RAG; 'llm' to never use RAG.
APP_RAG_MODE=auto
# Gating strategy to decide if RAG is needed: llm (default), heuristic, or hybrid
APP_RAG_GATE_STRATEGY=llm
# Minimum question length to consider RAG (in characters)
APP_RAG_MIN_QUESTION_LEN=12
# Heuristic keywords (used only if APP_RAG_GATE_STRATEGY=heuristic or hybrid)
APP_RAG_KEYWORDS=["onelitefeather","plugin","java","stacktrace","error","yaml","config.yml","plugin.yml",".yml",".java","github","release","build","gradle","maven","api","javadoc"]
# Optional: hard gating threshold (uses retrieval score). With similarity (higher better): use RAG if best_score >= threshold.
# With distance (lower better): use RAG if best_score <= threshold. If unset, keyword/length heuristics apply.
APP_RAG_GATE_THRESHOLD=

# --- Smalltalk configuration ---
# Exact smalltalk phrases that should be answered via plain LLM (JSON array)
APP_SMALLTALK_EXACT=["hi","hallo","hey","moin","servus","danke","thx","ok","yo","lol","danke!","merci","bitte","gern","gerne"]
# Phrases that, if contained in the message, indicate smalltalk (JSON array)
APP_SMALLTALK_CONTAINS=["guten morgen","guten abend","gute nacht","wie geht","wie läuft","alles gut","was geht","na?","moin moin","grüß","danke dir","vielen dank"]

# --- Job backend selection (for future Kubernetes) ---
# Choose: postgres (default), redis, or rabbitmq
APP_JOB_BACKEND=postgres
# Redis (when APP_JOB_BACKEND=redis)
APP_REDIS_URL=
APP_REDIS_NAMESPACE=rag
# RabbitMQ (when APP_JOB_BACKEND=rabbitmq)
APP_RABBITMQ_URL=
APP_RABBITMQ_QUEUE=rag_jobs
# --- Health server (for Kubernetes probes) ---
# Exposes GET /healthz and /readyz on the given port. Leave empty to disable.
APP_HEALTH_HTTP_PORT=
