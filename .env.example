# Copy to .env and fill in values. This file documents required variables.

# --- Discord Bot ---
# Discord Bot Token (create in Discord Developer Portal)
APP_DISCORD_TOKEN=
# Optional: custom presence text shown in Discord
APP_BOT_STATUS=OneLiteFeather RAG
# Optional: limit slash-command sync to specific guilds (JSON array, e.g. [123456789012345678])
APP_GUILD_IDS=
# Enable mention/reply triggers (requires enabling "Message Content Intent" in Discord Developer Portal)
APP_ENABLE_MESSAGE_CONTENT_INTENT=false

# --- RAG behavior ---
# If true, when no vectors are indexed yet, the bot answers via plain LLM
APP_RAG_FALLBACK_TO_LLM=true
# If true, when RAG finds no sources, include a general LLM answer appended to the RAG answer
APP_RAG_MIX_LLM_WITH_RAG=false
# If mixing is enabled, choose thresholding:
# When APP_RAG_SCORE_KIND=similarity (higher better), mix if best_score < threshold
# When APP_RAG_SCORE_KIND=distance (lower better), mix if best_score > threshold
APP_RAG_MIX_THRESHOLD=
APP_RAG_SCORE_KIND=similarity

# --- OpenAI ---
# OpenAI API Key used by the RAG service for LLM + embeddings
OPENAI_API_KEY=

# --- AI Provider ---
# Choose provider: openai (default) or ollama
APP_AI_PROVIDER=openai
# LLM + Embedding model names (provider-specific)
APP_LLM_MODEL=gpt-4.1-mini
APP_EMBED_MODEL=text-embedding-3-small
APP_TEMPERATURE=0.1
# Optional: set a system prompt for the LLM (also configurable via /config system_prompt_set)
APP_LLM_SYSTEM_PROMPT=

# Optional: choose embedding backend explicitly (openai, ollama, or vllm)
APP_EMBED_PROVIDER=openai

# Ollama specific (only used when APP_AI_PROVIDER=ollama)
# When using docker-compose with the provided service, use http://ollama:11434
APP_OLLAMA_BASE_URL=http://localhost:11434

# vLLM (OpenAI-compatible) provider
# Typical base URL: http://localhost:8000/v1 (used for LLM and, if APP_EMBED_PROVIDER=vllm, also for embeddings)
APP_VLLM_BASE_URL=http://localhost:8000/v1
# Some deployments require a key; otherwise leave empty
APP_VLLM_API_KEY=

# --- Postgres / pgvector (required for RAG API and indexing) ---
APP_PG_HOST=localhost
APP_PG_PORT=5432
APP_PG_USER=postgres
APP_PG_PASSWORD=postgres
APP_PG_DATABASE=postgres

# --- RAG settings ---
# Name of the pgvector table used for chunks
APP_TABLE_NAME=rag_chunks
# Embedding dimension for the selected embedding model (text-embedding-3-small -> 1536)
APP_EMBED_DIM=1536
# Top-k similar chunks to retrieve
APP_TOP_K=6

# --- Indexing CLI ---
# Trigger indexing via CLI, e.g.:
# uv run rag-index /path/to/repo https://github.com/ORG/repo
# or with Docker: docker compose run --rm bot rag-index /data/repos/my-repo https://github.com/ORG/my-repo

# --- Logging ---
# Default INFO; options: DEBUG, INFO, WARNING, ERROR
APP_LOG_LEVEL=INFO

# --- Ingestion defaults ---
# Default file extensions to include when none are provided by commands/config
# Provide as JSON array, e.g. [".md", ".py", ".yml", ".yaml", ".toml", ".json", ".txt"]
APP_INGEST_EXTS=[".md", ".py", ".yml", ".yaml", ".toml", ".json", ".txt"]
