# Copy to .env and fill in secrets/configs. This file is a template and should never be committed with real credentials.

# --- Discord Bot ---
# Discord bot token (keep secret!)
APP_DISCORD_TOKEN=
# Bot presence text shown in Discord
APP_BOT_STATUS="OneLiteFeather RAG"
# Optional allowlist; JSON array of guild IDs. Empty = global commands
APP_GUILD_IDS=[]
# Enable Message Content Intent (required for replies/mentions parsing)
APP_ENABLE_MESSAGE_CONTENT_INTENT=false

# Role-based admin control (optional, works in addition to Discord administrators)
APP_ADMIN_ROLE_IDS=[]
APP_ADMIN_ROLE_NAMES=[]

# --- AI provider selection ---
# Choose the LLM/embedding provider: openai | ollama | vllm
APP_AI_PROVIDER=openai
# Chat completion model (provider-specific name)
APP_LLM_MODEL=gpt-4.1-mini
# Embedding model and backend
APP_EMBED_MODEL=text-embedding-3-small
APP_EMBED_PROVIDER=openai  # openai|ollama|vllm
# Sampling temperature for LLM
APP_TEMPERATURE=0.1
# Optional base system prompt (leave empty to manage prompts in DB per scope)
APP_LLM_SYSTEM_PROMPT=

# Optional: override provider endpoints/keys
APP_OLLAMA_BASE_URL=http://localhost:11434
APP_VLLM_BASE_URL=http://localhost:8000/v1
APP_VLLM_API_KEY=
OPENAI_API_KEY=

# --- GitHub API (used by orgs/issues/listing) ---
# Optional; improves rate limits for GitHub ingestion (highly recommended)
GITHUB_TOKEN=
APP_GITHUB_COMMIT_METADATA=true  # collect per-file commit metadata during ingestion (disable to save API calls)

# --- Postgres / pgvector ---
APP_PG_HOST=localhost
APP_PG_PORT=5432
APP_PG_USER=postgres
APP_PG_PASSWORD=postgres
APP_PG_DATABASE=postgres

# --- RAG table & behavior ---
# Vector table name and embedding dimension
APP_TABLE_NAME=rag_chunks
APP_EMBED_DIM=1536
APP_TOP_K=6
# Behavior: fall back to plain LLM if no vectors exist
APP_RAG_FALLBACK_TO_LLM=true
# Optionally mix a plain LLM answer into the RAG answer
APP_RAG_MIX_LLM_WITH_RAG=false
APP_RAG_MIX_THRESHOLD=
APP_RAG_SCORE_KIND=similarity  # similarity|distance
# Gating (when to use RAG):
APP_RAG_MODE=auto  # auto|rag|llm
APP_RAG_GATE_STRATEGY=llm  # llm|heuristic|hybrid
APP_RAG_GATE_THRESHOLD=
# Minimum input length (characters) to consider answering
APP_RAG_MIN_QUESTION_LEN=12

# --- UI/messages (user-facing text; customizable) ---
# Extra style appended to the system prompt (e.g., tone, emoji usage). Prefer DB prompts for scope overrides.
APP_CHAT_STYLE_APPEND=
# Placeholder message shown while processing
APP_REPLY_PLACEHOLDER_TEXT="ðŸ§  Please wait â€” finding relevant context and preparing the answer â€¦"
# Language hint template when language is detected; {lang} will be replaced
APP_LANGUAGE_HINT_TEMPLATE="Response language: {lang}"
# Headings/prefixes used in responses
APP_SOURCES_HEADING="Sources:"
APP_REPLY_CONTEXT_LABEL="Context (previous bot message):"
APP_MEMORY_SUMMARY_HEADING="User profile (summary):"
APP_MEMORY_RECENT_HEADING="Recent conversation steps:"
APP_MEMORY_USER_PREFIX="User"
APP_MEMORY_BOT_PREFIX="Bot"

# --- Tools & Response policy ---
# Auto-plan and run tools based on natural language (admin-only)
APP_TOOLS_AUTO_ENABLE=true
# Choose how the bot responds: open threads for long or source-heavy answers
APP_POLICY_THREAD_ENABLE=true
APP_POLICY_THREAD_MIN_CHARS=400
APP_POLICY_THREAD_WHEN_SOURCES=true
APP_POLICY_THREAD_NAME_TEMPLATE="ðŸ”Ž {short_question}"
# Prefer replying to the user's message vs sending a channel message
APP_POLICY_REPLY_PREFER_REPLY=true
# Mention policy for replies/channel messages: never|auto|always
APP_POLICY_REPLY_MENTION=auto
# Show a placeholder message while computing the answer
APP_POLICY_USE_PLACEHOLDER=true

# --- Ingestion defaults ---
APP_INGEST_EXTS=[".md", ".py", ".yml", ".yaml", ".toml", ".json", ".txt", ".java"]

# --- Queue + ETL ---
# Job backend: postgres (local) or rabbitmq (recommended for scale)
APP_JOB_BACKEND=rabbitmq
# RabbitMQ AMQP URL when using rabbitmq (e.g., amqp://user:pass@localhost:5672/)
APP_RABBITMQ_URL=
# Default queue name; can be specialized per job type
APP_RABBITMQ_QUEUE=rag_jobs
# Worker poll interval (seconds)
APP_QUEUE_WATCH_POLL_SEC=5.0
# ETL staging (for manifests): backend + local dir or S3 settings
APP_ETL_STAGING_BACKEND=s3  # local | s3
APP_ETL_STAGING_DIR=.staging
APP_S3_STAGING_BUCKET=
APP_S3_STAGING_PREFIX=manifests
APP_S3_REGION=us-east-1
APP_S3_ENDPOINT_URL=
APP_S3_ACCESS_KEY_ID=
APP_S3_SECRET_ACCESS_KEY=

# --- LlamaIndex DocStore ---
# Persist LlamaIndex docstore/index metadata to local disk (separate from pgvector vectors)
APP_DOCSTORE_PERSIST=true
# Directory to store docstore/index metadata (JSON files)
APP_DOCSTORE_DIR=.staging/llama_storage
# DocStore backend: disk (default) or postgres (uses Postgres KV store)
APP_DOCSTORE_BACKEND=disk
# When using postgres backend, table name for KV storage
APP_DOCSTORE_TABLE=llama_kv

# --- LlamaIndex IndexStore ---
# Where to store LlamaIndex index structs (metadata about the index)
# postgres: store in Postgres JSONB table via KV store; disk: fallback to persist dir files
APP_INDEXSTORE_BACKEND=postgres
APP_INDEXSTORE_TABLE=llama_kv

# --- LlamaIndex ChatStore (SQLChatStore) ---
# Optional DSN override for ChatStore; by default the app Postgres settings are used
# Example: postgresql://user:pass@host:5432/database
APP_CHATSTORE_DSN=

# --- Health & metrics ---
# Expose /healthz, /readyz, /metrics via the internal HTTP server
APP_HEALTH_HTTP_PORT=8080

# --- Credits & Budgeting ---
# Enable credit/budget enforcement (false disables all checks)
APP_CREDIT_ENABLED=false
# Global monthly cap across all users (credits per month)
APP_CREDIT_GLOBAL_CAP=100000
# Default per-user monthly limit (overridden by ranks or per-user override)
APP_CREDIT_DEFAULT_LIMIT=1000
# Rank limits as JSON, e.g.: {"gold": 5000, "silver": 2000}
APP_CREDIT_RANK_LIMITS={}
# Map role name to rank as JSON, e.g.: {"Gold": "gold", "VIP": "gold"}
APP_CREDIT_ROLE_RANKS_BY_NAME={}
# Map role ID (string) to rank as JSON, e.g.: {"123456": "gold"}
APP_CREDIT_ROLE_RANKS_BY_ID={}
# Roles treated as unlimited (per-user); global cap still applies
APP_CREDIT_UNLIMITED_ROLE_NAMES=[]
APP_CREDIT_UNLIMITED_ROLE_IDS=[]
# Estimation parameters for pre-authorization
APP_CREDIT_TOKENS_PER_CHAR=0.25
APP_CREDIT_EST_OUTPUT_TOKENS=600
APP_CREDIT_PER_1K_TOKENS=1.0
# Label when mixing LLM response into a RAG answer (optional)
APP_RAG_MIX_LABEL=
