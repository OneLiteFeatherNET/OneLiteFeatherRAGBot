# Copy to .env and fill in values. This file documents required variables.

# --- Discord Bot ---
# Discord Bot Token (create in Discord Developer Portal)
APP_DISCORD_TOKEN=

# --- OpenAI ---
# OpenAI API Key used by the RAG service for LLM + embeddings
OPENAI_API_KEY=

# --- AI Provider ---
# Choose provider: openai (default) or ollama
APP_AI_PROVIDER=openai
# LLM + Embedding model names (provider-specific)
APP_LLM_MODEL=gpt-4.1-mini
APP_EMBED_MODEL=text-embedding-3-small
APP_TEMPERATURE=0.1

# Optional: choose embedding backend explicitly (openai or ollama)
APP_EMBED_PROVIDER=openai

# Ollama specific (only used when APP_AI_PROVIDER=ollama)
# When using docker-compose with the provided service, use http://ollama:11434
APP_OLLAMA_BASE_URL=http://localhost:11434

# vLLM (OpenAI-compatible) provider (only used when APP_AI_PROVIDER=vllm)
# Typical base URL: http://localhost:8000/v1
APP_VLLM_BASE_URL=http://localhost:8000/v1
# Some deployments require a key; otherwise leave empty
APP_VLLM_API_KEY=

# --- Postgres / pgvector (required for RAG API and indexing) ---
APP_PG_HOST=localhost
APP_PG_PORT=5432
APP_PG_USER=postgres
APP_PG_PASSWORD=postgres
APP_PG_DATABASE=postgres

# --- RAG settings ---
# Name of the pgvector table used for chunks
APP_TABLE_NAME=rag_chunks
# Embedding dimension for the selected embedding model (text-embedding-3-small -> 1536)
APP_EMBED_DIM=1536
# Top-k similar chunks to retrieve
APP_TOP_K=6

# --- Indexing CLI ---
# Trigger indexing via CLI, e.g.:
# uv run rag-index /path/to/repo https://github.com/ORG/repo
# or with Docker: docker compose run --rm bot rag-index /data/repos/my-repo https://github.com/ORG/my-repo

# --- Logging ---
# Default INFO; options: DEBUG, INFO, WARNING, ERROR
APP_LOG_LEVEL=INFO
